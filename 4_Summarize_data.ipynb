{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4- Summarize data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Malekkamoua/Medical-data-classifier/blob/main/4_Summarize_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxYBP_104dIb"
      },
      "source": [
        "from google.colab import drive\n",
        " \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymMGwX3A4kDs"
      },
      "source": [
        "!unzip  \"/content/gdrive/MyDrive/PFA DATA/Clean_data/Copy of clean_data.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwCtb0cI516V"
      },
      "source": [
        "import os \n",
        "from shutil import copyfile\n",
        "\n",
        "path = \"/content/content/supervised_sample\"\n",
        "os.mkdir(path)\n",
        "\n",
        "print(\"created \"+ path)\n",
        "\n",
        "count = 0\n",
        "for file in os.listdir(f\"/content/content/clean_data\"):\n",
        "  if count < 20000:\n",
        "    copyfile(f\"/content/content/clean_data/{file}\",f\"{path}/{file}\")\n",
        "    count  = count + 1\n",
        "\n",
        "print(count)\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOU_jIVy9SUo"
      },
      "source": [
        "pip install nltk==3.4.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkyDCE509Srg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d5ff550-8644-458e-fa30-8728591f9834"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm7xbYLo9ecy"
      },
      "source": [
        "!pip install sumy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YrrXTFf9iWo"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "def clean_text(x):\n",
        "    x = \" \".join(x.split())\n",
        "    x= \" \".join((\" \".join(x.split(\"[**\"))).split(\"**]\"))\n",
        "    x = re.sub(r\"\\([^()]*\\)\", \"\", x)\n",
        "    key_value_strip =(x.split(\":\"))\n",
        "    ##remove all sub strings which have a length lesser than 50 characters\n",
        "    string = \" \".join([sub_unit for sub_unit in key_value_strip if len(sub_unit)>50])\n",
        "    x = re.sub(r\"(\\d+)+(\\.|\\))\", \"\", string)## remove all serialization eg 1. 1)\n",
        "    x = re.sub(r\"(\\*|\\?|=)+\", \"\", x) ##removing all *, ? and =\n",
        "    x = re.sub(r\"\\b(\\w+)( \\1\\b)+\", r\"\\1\", x) ## removing consecutive dupicate words\n",
        "    x = x.replace(\"FOLLOW UP\", \"FOLLOWUP\")\n",
        "    x = x.replace(\"FOLLOW-UP\", \"FOLLOWUP\")\n",
        "    x = re.sub(r\"(\\b)(f|F)(irst)(\\b)?[\\d\\-\\d]*(\\s)*(\\b)?(n|N)(ame)[\\d\\-\\d]*(\\s)*[\\d\\-\\d]*(\\b)\",\"\",x)##remove firstname\n",
        "    x = re.sub(r\"(\\b)(l|L)(ast)(\\b)?[\\d\\-\\d]*(\\s)*(\\b)?(n|N)(ame)[\\d\\-\\d]*(\\s)*[\\d\\-\\d]*(\\b)\", \"\", x)\n",
        "    x = re.sub(r\"(\\b)(d|D)\\.?(r|R)\\.?(\\b)\", \"\", x) #remove Dr abreviation\n",
        "    x = re.sub(r\"([^A-Za-z0-9\\s](\\s)){2,}\", \"\", x)##remove consecutive punctuations\n",
        "    x = re.sub(r'\\d+', '', x)   #Remove numbers\n",
        "    x = x.lower() #To Lower case\n",
        "    return(x.replace(\"  \", \" \"))\n",
        "\n",
        "def ignore_non_ascii(text):\n",
        "  text = text.encode(\"ascii\", \"ignore\")\n",
        "  string_decode = text.decode()\n",
        "\n",
        "  return string_decode\n",
        "\n",
        "def only_english(text):\n",
        "\n",
        "  words = set(nltk.corpus.words.words())\n",
        "  clean = \" \".join(w for w in nltk.wordpunct_tokenize(text) if w.lower() in words or not w.isalpha())\n",
        "\n",
        "  return clean\n",
        "\n",
        "def only_period_punct (text):\n",
        "\n",
        "  punctuations = '''!()-[]{};:'\"\\,<>|°/?@#$%^&*_~©'''\n",
        "  no_punc = \"\"\n",
        "  for char in text:\n",
        "    if char not in punctuations:\n",
        "        no_punc = no_punc + char\n",
        "  \n",
        "  return no_punc\n",
        "\n",
        "def remove_long_words(text):\n",
        "  #The average word length in English language is 4.7 characters.\n",
        "  # remove words between 10 and 100\n",
        "  shortword = re.compile(r'\\W*\\b\\w{10,100}\\b')\n",
        "  return shortword.sub('', text)\n",
        "\n",
        "def remove_short_words(text):\n",
        "  #The average word length in English language is 4.7 characters.\n",
        "  # remove words between 1 and 2\n",
        "  shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
        "  return shortword.sub('', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gVJ-U2ZQw4m"
      },
      "source": [
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "\n",
        "# Import the LexRank summarizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "\n",
        "def summarize(text):\n",
        "\n",
        "  # Initializing the parser\n",
        "  my_parser = PlaintextParser.from_string(text,Tokenizer('english'))\n",
        "  # Creating a summary of 3 sentences.\n",
        "  lex_rank_summarizer = LexRankSummarizer()\n",
        "  lexrank_summary = lex_rank_summarizer(my_parser.document,sentences_count=3)\n",
        "\n",
        "  summary = \"\"\n",
        "  # Printing the summary\n",
        "  for sentence in lexrank_summary:\n",
        "    summary = summary + str(sentence)\n",
        "  \n",
        "  return summary\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWdgbSL69VU4"
      },
      "source": [
        "import re\n",
        "import string\n",
        "global str\n",
        "import nltk  \n",
        "from nltk.tokenize import word_tokenize "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTkBJ9oS9WwJ"
      },
      "source": [
        "# iterate through all files\n",
        "doc = []\n",
        "for file in os.listdir(path):\n",
        "    text = \" \"\n",
        "    if file.endswith(\".txt\"):\n",
        "        file_path = f\"{path}/{file}\"\n",
        "        with open(file_path, 'r') as f:\n",
        "          text = f.readline()\n",
        "\n",
        "          #preprocessing\n",
        "          res_clean_text = clean_text(text)\n",
        "          res_ignore_non_ascii = ignore_non_ascii(res_clean_text)\n",
        "          res_only_period_punct = only_period_punct(res_ignore_non_ascii)\n",
        "          res_only_english = only_english(res_only_period_punct)\n",
        "          res_remove_long_words = remove_long_words(res_only_english)\n",
        "          res_remove_short_words = remove_short_words(res_remove_long_words)\n",
        "\n",
        "          if res_remove_short_words !=  \"\":\n",
        "            doc.append(res_remove_short_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9WZZ8wmUnrK",
        "outputId": "f27e97b2-8a41-4d7a-ca51-c90068daeab3"
      },
      "source": [
        "len(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15545"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBELZXuKjZco"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "tmp = []\n",
        "summarized_data = []\n",
        "\n",
        "for file in doc:\n",
        "  #Removing stop words\n",
        "  stop_words = set(stopwords.words('english')) \n",
        "  word_tokens = word_tokenize(file)       \n",
        "  content = [w for w in word_tokens if not w in stop_words] \n",
        "  filtered_sentence = (\" \").join(content)\n",
        "  tmp.append(filtered_sentence)\n",
        "\n",
        "for file in tmp:\n",
        "  new_file = summarize(file)\n",
        "  summarized_data.append(new_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQf2KSJOlehD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e83cd935-4d86-4110-aeba-eeb85a4a6d3f"
      },
      "source": [
        "len(summarized_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15545"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    }
  ]
}